[MODEL_CONFIG]
max_grad_norm = 10
gamma = 1.0
lr_init = 5e-4
lr_min = 1e-5
lr_decay = linear
lr_ratio = 0.5
epsilon_init = 1.0
epsilon_min = 0.02
epsilon_decay = linear
epsilon_ratio = 0.3
num_fc = 64
num_h = 32
batch_size = 64
buffer_size = 2e5
reward_norm = 7314.15
reward_clip = 5.0


[TRAIN_CONFIG]
total_step = 10000
learning_starts = 1e3
train_freq = 1
target_network_update_freq = 1e3
num_update = 5
num_history = 20
test_freq = 10
print_freq = 10
log_freq = 10
rendering = 0
seed = 66


[ENV_CONFIG]
# the maximum step of one attempt
episode_length = 150
env_seed = 66
test_seeds = 10000,20000,30000
# use sample time: 0.05 or 0.01
sample_time = 0.05   
goal_x = 0.8
goal_y = 0.0
# set the target to be complete random if "random_field" is activated
random_field = False
radius_upper = 0.8
radius_lower = 0.3
adjacency_dis = 0.03
;reward coefficients
goal_coeff = 1
heading_coeff = 10
; for display
# draw = False
draw = True
